{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_from_fandom_page(category):\n",
    "\n",
    "        r = requests.get(category)\n",
    "        soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "        body = soup.find('body')\n",
    "        index = body.find('div', attrs={\"id\":\"main\"})\n",
    "        fandoms = index.find('ol')\n",
    "\n",
    "        boxes = fandoms.find_all('li', attrs = {'class':'letter listbox group'})\n",
    "\n",
    "        urls = []\n",
    "        for letter in boxes:\n",
    "                fandom = letter.find_all('li')\n",
    "                for each in fandom:\n",
    "                        url = each.find('a', attrs={'class':'tag'})\n",
    "                        urls.append(aothree+url['href'])\n",
    "                        \n",
    "        return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_works_from_page(soup):\n",
    "\n",
    "        main_body = soup.find('ol', attrs={'class':'work index group'})\n",
    "        works_raw = main_body.find_all('li', attrs={'class':'work blurb group'})\n",
    "        w_dict = {}\n",
    "        if works_raw: # if there are works on the page then do the work\n",
    "                df = pd.DataFrame()\n",
    "                for work in works_raw:\n",
    "                        \n",
    "                        header = work.find('h4')\n",
    "                        w_title = header.find('a').contents[0]\n",
    "\n",
    "                        #authors\n",
    "                        w_author = []\n",
    "                        raw_authors = header.find_all('a', attrs={'rel':'author'})\n",
    "                        for author in raw_authors:\n",
    "                                w_author.append(author.contents[0])\n",
    "                        #print(w_author)\n",
    "\n",
    "                        #fandoms\n",
    "                        fandom_heading = work.find('h5')\n",
    "                        w_fandom =[]\n",
    "                        raw_fandoms = fandom_heading.find_all('a', attrs={'class':'tag'})\n",
    "                        for fandom in raw_fandoms:\n",
    "                                w_fandom.append(fandom.contents[0])\n",
    "\n",
    "                        tag_header = work.find('ul', attrs={'class':'tags commas'})\n",
    "\n",
    "                        #Warnings\n",
    "                        warnings = tag_header.find_all('li', attrs={'class':'warnings'})\n",
    "                        w_warnings = []\n",
    "                        for warn in warnings:\n",
    "                                raw_warn = warn.find('a')\n",
    "                                w_warnings.append(raw_warn.contents[0]) \n",
    "                        #pairings\n",
    "                        pairings = tag_header.find_all('li', attrs={'class':'relationships'})\n",
    "                        w_pairings = []\n",
    "                        for pair in pairings:\n",
    "                                raw_pair = pair.find('a').contents[0]\n",
    "                                w_pairings.append(raw_pair)\n",
    "\n",
    "                         #characters\n",
    "                        characters = tag_header.find_all('li', attrs={'class':'characters'})\n",
    "                        w_chars = []\n",
    "                        for char in characters:\n",
    "                                raw_char = char.find('a').contents[0]\n",
    "                                w_chars.append(raw_char)\n",
    "\n",
    "                        #freeforms\n",
    "                        freeform = tag_header.find_all('li', attrs={'class':'freeforms'})\n",
    "                        w_freeform = []\n",
    "                        for free in freeform:\n",
    "                                raw_free = free.find('a').contents[0]\n",
    "                                w_freeform.append(raw_free)\n",
    " \n",
    "                        #ADD EXCEPTIONS FOR EVERY CASE WHERE THERE MAY NOT BE A STAT!     \n",
    " \n",
    "                        #stats\n",
    "                        stats = work.find('dl', attrs={'class':'stats'})\n",
    "\n",
    "                        #words\n",
    "                        if stats.find('dd', attrs = {'class':'words'}).contents:\n",
    "                            w_words = stats.find('dd', attrs={'class':'words'}).contents\n",
    "                        else:\n",
    "                            w_words = np.nan\n",
    "\n",
    "                        #chapters\n",
    "                        w_chaps = stats.find('dd', attrs={'class':'chapters'}).contents[0]\n",
    "\n",
    "                        #Comments\n",
    "                        if stats.find('dd', attrs={'class':'comments'}):\n",
    "                                w_comments = stats.find('dd', attrs={'class':'comments'}).a.contents[0]\n",
    "                        else:\n",
    "                                w_comments = np.nan\n",
    "                        #kudos\n",
    "                        if stats.find('dd', attrs={'class':'kudos'}):\n",
    "                                w_kudos = stats.find('dd', attrs={'class':'kudos'}).a.contents[0]\n",
    "                        else:\n",
    "                                w_kudos = np.nan\n",
    "\n",
    "                        #bookmarks\n",
    "                        if stats.find('dd', attrs={'class':'bookmarks'}):\n",
    "                                w_bookmark = stats.find('dd', attrs={'class':'bookmarks'}).a.contents[0]\n",
    "                        else:\n",
    "                                w_bookmark = np.nan\n",
    "\n",
    "                        #hits\n",
    "                        if stats.find('dd', attrs={'class':'hits'}):\n",
    "                                w_hits = stats.find('dd', attrs={'class':'hits'}).contents[0]\n",
    "                        else:\n",
    "                                w_hits = np.nan\n",
    "\n",
    "                        w_dict = {'Title': w_title, 'Authors':w_author, 'Fandoms':w_fandom, \\\n",
    "                                   'Warnings':w_warnings, 'Pairings': w_pairings, \\\n",
    "                                   'Characters':w_chars ,'Freeforms':w_freeform, \\\n",
    "                                   'Words':w_words, 'Chapters':w_chaps,\\\n",
    "                                   'Kudos':w_kudos, 'Bookmarks':w_bookmark, 'Hits':w_hits, \\\n",
    "                                   'Comments':w_comments}\n",
    "        \n",
    "                        df = df.append(w_dict, ignore_index = True)\n",
    "       \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aothree = \"https://archiveofourown.org\"      \n",
    "fandomcates = ['https://archiveofourown.org/media/Anime%20*a*%20Manga/fandoms', \n",
    "               'https://archiveofourown.org/media/Books%20*a*%20Literature/fandoms', \n",
    "               'https://archiveofourown.org/media/Cartoons%20*a*%20Comics%20*a*%20Graphic%20Novels/fandoms', \n",
    "               'https://archiveofourown.org/media/Celebrities%20*a*%20Real%20People/fandoms', \n",
    "               'https://archiveofourown.org/media/Movies/fandoms', \n",
    "               'https://archiveofourown.org/media/Music%20*a*%20Bands/fandoms', \n",
    "               'https://archiveofourown.org/media/Other%20Media/fandoms', \n",
    "               'https://archiveofourown.org/media/Theater/fandoms', \n",
    "               'https://archiveofourown.org/media/TV%20Shows/fandoms', \n",
    "               'https://archiveofourown.org/media/Video%20Games/fandoms', \n",
    "               'https://archiveofourown.org/media/Uncategorized%20Fandoms/fandoms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cat = fandomcates[10]\n",
    "#store = pd.HDFStore('aothree.h5')\n",
    "#urls = get_list_from_fandom_page(cat) #returns list of urls for each fandom\n",
    "\n",
    "#write urls to textfiles.\n",
    "#f = open(\"uncategorized.txt\", 'w')\n",
    "#for fandom in urls:\n",
    "#    f.write(\"{}\\n\".format(fandom))\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for fandom in urls:\n",
    "    print(fandom)\n",
    "    eof = False #end of fandom\n",
    "    while eof == False:\n",
    "        purl = ''\n",
    "        r = requests.get(fandom)\n",
    "        soup = BeautifulSoup(r.content, 'lxml')\n",
    "        #get the data off the page\n",
    "        df = get_works_from_page(soup)\n",
    "        #df.to_hdf('aothree.h5','Works', append=True)\n",
    "        df.to_csv('aothree.csv', mode = 'a', header = False)\n",
    "        \n",
    "        #df.to_csv(f, header=False)\n",
    "        lo = soup.find('li', attrs = {'class':'next'})\n",
    "        if lo !=None:\n",
    "            if lo.find('a') != None:\n",
    "                purl = lo.find('a')['href']\n",
    "            else:\n",
    "                eof = True\n",
    "        else:\n",
    "            eof = True\n",
    "        fandom = aothree+purl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = 'Maybe'\n",
    "l2 = ['Great', 'Angel']\n",
    "l3 = []\n",
    "l3.append(l1)\n",
    "print(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only grab the anime fics\n",
    "works_url = urls[0][0]\n",
    "eof = False #end of fandom?\n",
    "url = works_url\n",
    "while eof == False:\n",
    "    purl = ''\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    lo = soup.find('li', attrs = {'class':'next'})\n",
    "    if lo.find('a') != None:\n",
    "        purl = lo.find('a')['href']\n",
    "        print(purl)\n",
    "    else:\n",
    "        eof = True\n",
    "    url = aothree+purl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
